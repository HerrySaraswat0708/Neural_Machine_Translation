{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a4bbba-b093-4b00-b6ca-754fe6d81289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b655d438-d470-40ec-975c-bb56d1d2594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff1f980-f2a9-4aa5-a7d8-25250119f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea2fe33-d426-4e22-8e03-27fcae797d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeEnglish(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)            \n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)        \n",
    "    return s.strip()\n",
    "\n",
    "def normalizeFrench(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñæœ .!?'-]+\", r\" \", s)  \n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdceabb-0794-43e5-8cb2-91e642914181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2):\n",
    "    lines = open(\"C:\\\\Users\\\\harry\\\\Desktop\\\\eng-fra.txt\", encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    if lang1 == 'eng':\n",
    "        normalize1 = normalizeEnglish\n",
    "    else:\n",
    "        normalize1 = normalizeFrench \n",
    "\n",
    "    if lang2 == 'fra':\n",
    "        normalize2 = normalizeFrench\n",
    "    else:\n",
    "        normalize2 = normalizeEnglish\n",
    "        \n",
    "    pairs = [[normalize1(s.split('\\t')[0]), normalize2(s.split('\\t')[1])] for s in lines]\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec6e75d-3ef1-492c-93b4-b49fbcc827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 16\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1491b0d-f77c-452e-b00d-03a10ed5b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    random.shuffle(pairs)\n",
    "    train_pair = pairs[:round(len(pairs)*0.8)]\n",
    "    test_pair = pairs[round(len(pairs)*0.8):]\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, train_pair, test_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ef43e6-c0e9-4779-b451-5e7457e081c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12596\n",
      "fra 26188\n"
     ]
    }
   ],
   "source": [
    "input_lang,output_lang,train_pair,test_pair = prepareData('eng','fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0d34f8-01a4-42df-9afb-1639b973ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9454aff1-a537-4486-83b9-6137ca71032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def train_dataloader(batch_size, input_lang, output_lang, pairs, max_length=MAX_LENGTH):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.full((n, max_length), PAD_token, dtype=np.int32)\n",
    "    target_ids = np.full((n, max_length), PAD_token, dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)[:max_length - 1]\n",
    "        tgt_ids = [SOS_token] + indexesFromSentence(output_lang, tgt)[:max_length - 2]\n",
    "\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_ids).to(device)\n",
    "    target_tensor = torch.LongTensor(target_ids).to(device)\n",
    "\n",
    "    train_data = TensorDataset(input_tensor, target_tensor)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14659d0-902c-43ea-8307-e10b6828011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def test_dataloader(batch_size, input_lang, output_lang, pairs, max_length=MAX_LENGTH):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.full((n, max_length), PAD_token, dtype=np.int32)\n",
    "    target_ids = np.full((n, max_length), PAD_token, dtype=np.int32)\n",
    "    \n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)[:max_length - 1]\n",
    "        tgt_ids = [SOS_token] + indexesFromSentence(output_lang, tgt)[:max_length - 2]\n",
    "\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_ids).to(device)\n",
    "    target_tensor = torch.LongTensor(target_ids).to(device)\n",
    "    # print(len(input_tensor))\n",
    "    # print(len(target_tensor))\n",
    "    test_data = TensorDataset(input_tensor, target_tensor)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # print(len(test_data))\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ef5720-27d1-49c0-93e9-7c96a1c52886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 32\n",
    "eval_pair = test_pair\n",
    "train_pair = train_pair\n",
    "train_dataloader = train_dataloader(batch_size,input_lang,output_lang,train_pair)\n",
    "eval_dataloader = test_dataloader(batch_size,input_lang,output_lang,eval_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c22db9a-9cbd-48db-8dc2-882e8df63232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(829, 3318)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataloader),len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b8fedd-163e-489f-8831-735484da2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_q = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
    "        self.W_k = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
    "        self.W_v = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "        attention = torch.softmax(Q@K.transpose(1,2)/self.emb_dim**0.5, dim=-1)\n",
    "        out = attention@V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3efe3d-c590-4df4-ac46-bfde88943083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.W_0 = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, q,k,v, mask=None):\n",
    "        batch_size, seq_len, emb_dim = q.shape\n",
    "        H = self.num_heads\n",
    "        D = self.head_dim\n",
    "\n",
    "        Q = self.W_q(q).reshape(batch_size, -1, H, D).transpose(1, 2)\n",
    "        K = self.W_k(k).reshape(batch_size, -1, H, D).transpose(1, 2)\n",
    "        V = self.W_v(v).reshape(batch_size, -1, H, D).transpose(1, 2)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / (D ** 0.5) \n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            # print('mask :',mask.shape)\n",
    "            # print('score:',scores.shape)\n",
    "            scores = scores.masked_fill(mask == 1, value=-1e9)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ V \n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, emb_dim)\n",
    "        output_final = self.W_0(out)\n",
    "        return output_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97dcabd4-55bb-490d-acd5-4ab33654e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,emb_dim):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, emb_dim, 2, dtype=torch.float) * -(np.log(10000.0) / emb_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # takes vectors from 0 with step of 2 \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # takes vectors from 1 with step of 2 \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab067c0c-8db7-401e-99a1-053fca412841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        nn.init.normal_(self.token_emb.weight, mean=0, std=0.1)\n",
    "        self.pos_emb = PositionalEncoding(max_seq_len, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b835a4d7-8024-499d-87a7-754decae079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(emb_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c36322-25de-4339-beca-60ff5d2121eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0750e70-faf0-41ac-946b-1769ee19dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.addnorm1 = AddNorm(emb_dim, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(emb_dim, ff_dim, dropout)\n",
    "        self.addnorm2 = AddNorm(emb_dim, dropout)\n",
    "\n",
    "    def forward(self, x, enc_mask=None):\n",
    "        x = self.addnorm1(x, self.mha(x,x,x,enc_mask))\n",
    "        x = self.addnorm2(x, self.ffn(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65f1c9ea-809f-481d-b214-b4fdbf4a4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.addnorm1 = AddNorm(emb_dim, dropout)\n",
    "        \n",
    "        self.cross_mha = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.addnorm2 = AddNorm(emb_dim, dropout)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(emb_dim, ff_dim, dropout)\n",
    "        self.addnorm3 = AddNorm(emb_dim, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, enc_mask=None):\n",
    "        # Masked self-attention\n",
    "        x = self.addnorm1(x, self.masked_mha(x, x, x, tgt_mask))\n",
    "\n",
    "        # Encoder-decoder cross attention\n",
    "        x = self.addnorm2(x, self.cross_mha(x, enc_out, enc_out, enc_mask))\n",
    "\n",
    "        # Feed-forward\n",
    "        x = self.addnorm3(x, self.ffn(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c9af0e5-bc10-4d5e-8ce1-bba09bdc7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(src, pad_idx=0):\n",
    "    return (src == pad_idx).type(torch.int16).unsqueeze(-2).unsqueeze(-2)\n",
    "\n",
    "def generate_subsequent_mask(seq_len, device=None):\n",
    "    mask = torch.triu(torch.ones((1,seq_len, seq_len)),diagonal=1).type(torch.int16)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ac8cc0-d982-4a37-aa36-75ee712d4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=512, num_heads=8,\n",
    "                 ff_dim=2048, num_layers=6, max_seq_len=256, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.src_embed = TransformerEmbedding(src_vocab_size, emb_dim, max_seq_len)\n",
    "        self.tgt_embed = TransformerEmbedding(tgt_vocab_size, emb_dim, max_seq_len)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(emb_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(emb_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(emb_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = (src == self.pad_idx).unsqueeze(-2).unsqueeze(-2).to(src.device)  \n",
    "        tgt_mask = generate_subsequent_mask(tgt.shape[1]).to(tgt.device)     \n",
    "        tgt_pad_mask = (tgt == self.pad_idx).unsqueeze(-2).to(tgt.device)  \n",
    "        combined_tgt_mask = tgt_pad_mask | tgt_mask \n",
    "        combined_tgt_mask = combined_tgt_mask.unsqueeze(1)\n",
    "    \n",
    "        # Embedding\n",
    "        src_emb = self.src_embed(src)  \n",
    "        tgt_emb = self.tgt_embed(tgt)  \n",
    "    \n",
    "        # Encoder\n",
    "        enc_out = src_emb\n",
    "        for layer in self.encoder:\n",
    "            enc_out = layer(enc_out, src_mask)\n",
    "    \n",
    "        # Decoder\n",
    "        dec_out = tgt_emb\n",
    "        for layer in self.decoder:\n",
    "            dec_out = layer(dec_out, enc_out, combined_tgt_mask, src_mask)\n",
    "    \n",
    "        # Final projection to vocabulary size\n",
    "        output = self.output_proj(dec_out)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9312b7dc-58fd-46cb-8495-fb93e430a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(input_lang.n_words,\n",
    "                    output_lang.n_words,\n",
    "                    emb_dim=128,\n",
    "                    num_heads=8,\n",
    "                    ff_dim=2048,\n",
    "                    num_layers=6,\n",
    "                    max_seq_len=MAX_LENGTH,\n",
    "                    dropout=0.1,\n",
    "                    pad_idx=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "174b0e62-b2ac-43e2-aa03-30da23dff900",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e922f34e-0d5f-4659-8883-05478599c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, pad_idx, device, clip=1.0):\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        total_correct = 0\n",
    "        for src, tgt in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "          \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_target = tgt[:, 1:]             \n",
    "            \n",
    "        \n",
    "            outputs = model(src, tgt_input)  \n",
    "            logits = outputs.reshape(-1, outputs.size(-1))\n",
    "            tgt_flat = tgt_target.reshape(-1)\n",
    "        \n",
    "            loss = criterion(logits, tgt_flat)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = logits.argmax(dim=-1)                        \n",
    "            non_pad_mask = tgt_flat != pad_idx\n",
    "            correct = (preds == tgt_flat) & non_pad_mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_tokens += non_pad_mask.sum().item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * total_correct / total_tokens if total_tokens > 0 else 0\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e2d26f9-6b8b-4b3f-9bf7-478d23c5080f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss: 4.4494 | Accuracy: 39.11%\n",
      "Epoch  2 | Loss: 3.0549 | Accuracy: 51.97%\n",
      "Epoch  3 | Loss: 2.4110 | Accuracy: 59.74%\n",
      "Epoch  4 | Loss: 1.9909 | Accuracy: 65.07%\n",
      "Epoch  5 | Loss: 1.7010 | Accuracy: 68.67%\n",
      "Epoch  6 | Loss: 1.4885 | Accuracy: 71.25%\n",
      "Epoch  7 | Loss: 1.3214 | Accuracy: 73.37%\n",
      "Epoch  8 | Loss: 1.1863 | Accuracy: 75.13%\n",
      "Epoch  9 | Loss: 1.0745 | Accuracy: 76.60%\n",
      "Epoch 10 | Loss: 0.9775 | Accuracy: 77.93%\n",
      "Epoch 11 | Loss: 0.8954 | Accuracy: 79.09%\n",
      "Epoch 12 | Loss: 0.8220 | Accuracy: 80.19%\n",
      "Epoch 13 | Loss: 0.7570 | Accuracy: 81.15%\n",
      "Epoch 14 | Loss: 0.6987 | Accuracy: 82.12%\n",
      "Epoch 15 | Loss: 0.6469 | Accuracy: 82.96%\n",
      "Epoch 16 | Loss: 0.6018 | Accuracy: 83.73%\n",
      "Epoch 17 | Loss: 0.5587 | Accuracy: 84.49%\n",
      "Epoch 18 | Loss: 0.5219 | Accuracy: 85.19%\n",
      "Epoch 19 | Loss: 0.4876 | Accuracy: 85.86%\n",
      "Epoch 20 | Loss: 0.4573 | Accuracy: 86.44%\n"
     ]
    }
   ],
   "source": [
    "train_epoch(model, train_dataloader, optimizer, criterion, PAD_token, device, clip=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e28795-6c5a-40e6-9f3d-b49c96b9664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'trans.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73e4b899-dce3-494e-8105-1b1a7bb5d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('trans.pth',weights_only=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20b1c4fd-4799-4852-8994-8e4e50856d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, criterion, pad_idx, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_target = tgt[:, 1:]\n",
    "\n",
    "        outputs = model(src, tgt_input)                     # [B, T, V]\n",
    "        logits = outputs.reshape(-1, outputs.size(-1))      # [B*T, V]\n",
    "        tgt_flat = tgt_target.reshape(-1)                   # [B*T]\n",
    "\n",
    "        loss = criterion(logits, tgt_flat)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = logits.argmax(dim=-1)                       # [B*T]\n",
    "        non_pad_mask = tgt_flat != pad_idx\n",
    "        correct = (preds == tgt_flat) & non_pad_mask\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * total_correct / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    print(f\"[Eval] Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "    # return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75f573b0-8a71-4853-af82-28f8afab6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Loss: 1.2688 | Accuracy: 77.55%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eval_dataloader, criterion, PAD_token, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1256b61a-8a23-4a8b-94cb-998cb3494c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_tensor, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tgt_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            out = model(src_tensor, tgt_input)\n",
    "            next_token_logits = out[:, -1, :]  # (batch, vocab)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)  # (batch, 1)\n",
    "\n",
    "            tgt_input = torch.cat([tgt_input, next_token], dim=1)\n",
    "            if next_token.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "        output_indexes = tgt_input.squeeze().tolist()[1:]  # Remove SOS token\n",
    "        output_words = [output_lang.index2word[idx] for idx in output_indexes if idx not in [EOS_token, PAD_token]]\n",
    "        return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c428730b-8741-43df-acd4-ec1bc01c88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text,model,input_lang,output_lang,max_length):\n",
    "    \n",
    "    text = normalizeEnglish(text)\n",
    "    input_ids = np.full((1, max_length), PAD_token, dtype=np.int32)\n",
    "    inp_ids = indexesFromSentence(input_lang, text)[:max_length - 1] \n",
    "    inp_ids.append(EOS_token)\n",
    "    input_ids[0,:len(inp_ids)] = inp_ids\n",
    "    input_tensor = torch.LongTensor(input_ids)\n",
    "    \n",
    "    print('French sentence : ',greedy_decode(model, input_tensor, input_lang, output_lang, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3fca069-52ac-4093-89d8-c47fe8ee56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "English sentence :  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentence :  écoute  !\n"
     ]
    }
   ],
   "source": [
    "text = input('English sentence : ')\n",
    "generate(text,model,input_lang,output_lang,MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456fe9b-f515-4cc9-af04-21e1d3520dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
